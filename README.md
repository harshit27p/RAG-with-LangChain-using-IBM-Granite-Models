# ğŸ” Retrieval Augmented Generation (RAG) with LangChain using IBM Granite Models

Welcome to the **Retrieval Augmented Generation (RAG)** lab using **LangChain**, **IBM Granite LLMs**, and a vector database (**Milvus**). This project demonstrates how to augment LLMs by retrieving external contextual data to generate more factual, relevant, and accurate responses.

---

## ğŸš€ Overview

**Retrieval Augmented Generation (RAG)** enhances Large Language Models by integrating a document retrieval step before generating responses. It allows LLMs to "look up" facts from a knowledge base rather than relying solely on pre-trained data.

This lab:
- Indexes a text document using embeddings
- Stores the vectors in Milvus DB
- Retrieves relevant chunks at query time
- Feeds those chunks to a Granite model on Replicate for response generation

---

## ğŸ§  Use Cases

- âœ… Customer Support with contextual documents  
- âœ… Internal Knowledge Base Q&A  
- âœ… Domain-specific assistants (Finance, Legal, Healthcare)  
- âœ… Chatbots with real-time document grounding

---

## ğŸ§± Architecture

```text
           +--------------------+
           |    User Query      |
           +--------------------+
                    |
                    v
         +------------------------+
         |   Embedding Query      |
         +------------------------+
                    |
                    v
         +------------------------+
         |  Vector DB (Milvus)    |
         +------------------------+
                    |
                    v
         +------------------------+
         | Retrieve Top-K Chunks |
         +------------------------+
                    |
                    v
         +------------------------+
         |  IBM Granite LLM (RAG) |
         +------------------------+
                    |
                    v
         +------------------------+
         |   Final Generated Answer |
         +------------------------+
```

---
## ğŸ“ Folder Structure
```
.
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ RAG_with_LangChain.ipynb       # Main Jupyter Notebook for the lab
â”œâ”€â”€ data/
â”‚   â””â”€â”€ state_of_the_union.txt         # Sample knowledge base
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ requirements.txt               # Dependency list
â”œâ”€â”€ assets/
â”‚   â””â”€â”€ architecture.png               # Diagram/image (optional)
â”œâ”€â”€ .env.example                       # Example for setting environment variables
â””â”€â”€ README.md                          # You're here!
```
---
## âš™ï¸ Requirements

   - Python 3.10, 3.11, or 3.12
   - Google Colab or local environment
   - Replicate API Key
   - Dockerhub account (optional)
   - GitHub account

---
## âœ… Key Features Demonstrated

    ğŸ”¹ Document chunking using langchain.text_splitter

    ğŸ”¹ Embeddings generation with IBM Granite Embedding Model

    ğŸ”¹ Vector similarity search with Milvus

    ğŸ”¹ Querying LLM with retrieved context using LangChain RAG chain

    ğŸ”¹ Full pipeline orchestration with LangChain

---
## ğŸ“š Resources

    ğŸ”— IBM Granite Models on HuggingFace

    ğŸ”— LangChain Documentation

    ğŸ”— Milvus Vector Database

    ğŸ”— IBM Granite RAG Notebook (Colab)

---
## ğŸ”— Access This Lab Online

â–¶ï¸ Run the RAG with LangChain Notebook in Colab:https://colab.research.google.com/github/ibm-granite-community/granite-snack-cookbook/blob/main/recipes/RAG/RAG_with_Langchain.ipynb#scrollTo=mCXAgbCHXxlH

ğŸ“˜ Official IBM Skills Network Lab:https://skills.yourlearning.ibm.com/activity/ALM-COURSE_3824998?ngo-id=0330&utm_campaign=aca-Edunet-GLERAGLAB-2025-26

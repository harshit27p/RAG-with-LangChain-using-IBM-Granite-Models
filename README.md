# ğŸ” Retrieval Augmented Generation (RAG) with LangChain using IBM Granite Models

Welcome to the **Retrieval Augmented Generation (RAG)** lab using **LangChain**, **IBM Granite LLMs**, and a vector database (**Milvus**). This project demonstrates how to augment LLMs by retrieving external contextual data to generate more factual, relevant, and accurate responses.

---

## ğŸš€ Overview

**Retrieval Augmented Generation (RAG)** enhances Large Language Models by integrating a document retrieval step before generating responses. It allows LLMs to "look up" facts from a knowledge base rather than relying solely on pre-trained data.

This lab:
- Indexes a text document using embeddings
- Stores the vectors in Milvus DB
- Retrieves relevant chunks at query time
- Feeds those chunks to a Granite model on Replicate for response generation

---

## ğŸ§  Use Cases

- âœ… Customer Support with contextual documents  
- âœ… Internal Knowledge Base Q&A  
- âœ… Domain-specific assistants (Finance, Legal, Healthcare)  
- âœ… Chatbots with real-time document grounding

---

## ğŸ§± Architecture

```text
           +--------------------+
           |    User Query      |
           +--------------------+
                    |
                    v
         +------------------------+
         |   Embedding Query      |
         +------------------------+
                    |
                    v
         +------------------------+
         |  Vector DB (Milvus)    |
         +------------------------+
                    |
                    v
         +------------------------+
         | Retrieve Top-K Chunks |
         +------------------------+
                    |
                    v
         +------------------------+
         |  IBM Granite LLM (RAG) |
         +------------------------+
                    |
                    v
         +------------------------+
         |   Final Generated Answer |
         +------------------------+
```

---
## ğŸ“ Folder Structure
```
.
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ RAG_with_LangChain.ipynb       # Main Jupyter Notebook for the lab
â”œâ”€â”€ data/
â”‚   â””â”€â”€ state_of_the_union.txt         # Sample knowledge base
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ requirements.txt               # Dependency list
â”œâ”€â”€ assets/
â”‚   â””â”€â”€ architecture.png               # Diagram/image (optional)
â”œâ”€â”€ .env.example                       # Example for setting environment variables
â””â”€â”€ README.md                          # You're here!
```
---
## âš™ï¸ Requirements

   - Python 3.10, 3.11, or 3.12
   - Google Colab or local environment
   - Replicate API Key
   - Dockerhub account (optional)
   - GitHub account

---

## ğŸ§ª Installation
ğŸ‘‰ Option 1: Run on Google Colab
ğŸ‘‰ Option 2: Run locally

### Clone the repository
git clone https://github.com/yourusername/rag-granite-langchain.git
cd rag-granite-langchain

### Create and activate a virtual environment
python -m venv .venv
source .venv/bin/activate  # or .venv\Scripts\activate on Windows

### Install requirements
pip install -r scripts/requirements.txt

## ğŸ” Environment Variables

Create a .env file based on .env.example and set your Replicate API key:

REPLICATE_API_TOKEN=your_replicate_token_here

### ğŸ› ï¸ Run the Lab

Inside the virtual environment
jupyter notebook notebooks/RAG_with_LangChain.ipynb

---
## âœ… Key Features Demonstrated

    ğŸ”¹ Document chunking using langchain.text_splitter

    ğŸ”¹ Embeddings generation with IBM Granite Embedding Model

    ğŸ”¹ Vector similarity search with Milvus

    ğŸ”¹ Querying LLM with retrieved context using LangChain RAG chain

    ğŸ”¹ Full pipeline orchestration with LangChain

---
## ğŸ“š Resources

    ğŸ”— IBM Granite Models on HuggingFace

    ğŸ”— LangChain Documentation

    ğŸ”— Milvus Vector Database

    ğŸ”— IBM Granite RAG Notebook (Colab)

